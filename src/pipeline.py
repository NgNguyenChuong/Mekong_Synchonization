import os
import time
from concurrent.futures import ProcessPoolExecutor
from config import (
    DATA_SPECS, H3_GRID_GEOJSON, CRS_METRIC, CRS_WGS84
)
from utils_h3 import load_h3_multipoints
from preprocessing import run_preprocessing
from processing import process_single_dataset, merge_all_datasets # <--- ThÃªm merge_all_datasets
def main():
    start_time = time.time()
    print("==================================================")
    print("ðŸš€ STARTING PARALLEL H3 PIPELINE")
    print("==================================================")
    
    # 1. PREPROCESSING (Váº«n cháº¡y tuáº§n tá»± vÃ¬ cáº§n file nÃ y Ä‘á»ƒ cháº¡y tiáº¿p)
    try:
        run_preprocessing()
    except Exception as e:
        print(f"âŒ Preprocessing failed: {e}")
        return

    # 2. LOAD GRID (Load 1 láº§n á»Ÿ Main Process)
    print("\n[STEP 1] Loading H3 Grid Geometry...")
    if not os.path.exists(H3_GRID_GEOJSON):
        print("âŒ Grid file missing.")
        return

    # H3 Data Bundle sáº½ Ä‘Æ°á»£c copy sang cÃ¡c process con (nhá» cÆ¡ cháº¿ cá»§a Python)
    h3_data_bundle = load_h3_multipoints(H3_GRID_GEOJSON, CRS_METRIC, CRS_WGS84)
    print(f"âœ… Loaded {len(h3_data_bundle[0])} cells. Ready to fork.")

    # 3. PARALLEL PROCESSING
    print("\n[STEP 2] Launching Parallel Workers...")
    
    # Chuáº©n bá»‹ danh sÃ¡ch tham sá»‘ Ä‘á»ƒ Ä‘áº©y vÃ o Pool
    # Má»—i worker cáº§n: key, config cá»§a key Ä‘Ã³, vÃ  dá»¯ liá»‡u grid
    tasks = []
    for key, spec in DATA_SPECS.items():
        # Kiá»ƒm tra folder input cÃ³ tá»“n táº¡i khÃ´ng trÆ°á»›c khi Ä‘Æ°a vÃ o queue
        # (Äá»ƒ trÃ¡nh lá»—i váº·t)
        tasks.append((key, spec, h3_data_bundle))

    # Sá» LUá»’NG Tá»I ÄA (max_workers)
    # Náº¿u mÃ¡y báº¡n 8 core, Ä‘á»ƒ 4-6 lÃ  Ä‘áº¹p. Náº¿u RAM yáº¿u thÃ¬ giáº£m xuá»‘ng.
    # Máº·c Ä‘á»‹nh None = sá»‘ core cá»§a mÃ¡y.
    MAX_WORKERS = os.cpu_count() - 1  # Chá»«a 1 core cho há»‡ thá»‘ng
    
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Map hÃ m worker vá»›i danh sÃ¡ch tasks
        results = list(executor.map(process_single_dataset, tasks))
    
    # --- MERGE CSV ---
    print("\n[STEP 3] Merging all datasets...")
    merge_all_datasets()
    # ---------------------

    print("\n==================================================")
    print(f"âœ… PIPELINE FINISHED in {time.time() - start_time:.1f} seconds")
    print("==================================================")

if __name__ == "__main__":
    # Báº®T BUá»˜C PHáº¢I CÃ“ DÃ’NG NÃ€Y TRÃŠN WINDOWS Äá»‚ CHáº Y MULTIPROCESSING
    main()