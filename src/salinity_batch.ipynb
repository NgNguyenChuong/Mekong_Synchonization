{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe34cf77",
   "metadata": {},
   "source": [
    "# Batch t√≠nh ƒë·ªô m·∫∑n (salinity) cho c√°c raster Landsat 2022\n",
    "X·ª≠ l√Ω t·∫•t c·∫£ file .tif trong `data/processed/landsat_salinity_2022_full_year` v·ªõi H3 grid ƒë√£ l√†m s·∫°ch.\n",
    "- T·∫°o H3 grid (res 7) v·ªõi chi·∫øn l∆∞·ª£c Buffer ‚Üí Polyfill ‚Üí Intersect\n",
    "- Lo·∫°i b·ªè ƒë·∫£o nh·ªè <600 km¬≤ t·ª´ ranh gi·ªõi ƒêBSCL\n",
    "- T√≠nh salinity features: min/max/mean/std + % pixel ‚â• 0.2/0.5/1.0 ppt + ph√¢n c·∫•p r·ªßi ro\n",
    "- ƒê·∫ßu ra: `salinity_h3_features_2022.csv` trong c√πng th∆∞ m·ª•c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fadd8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: d:\\Mekong_DGGS\n",
      "‚úÖ Th∆∞ m·ª•c salinity: d:\\Mekong_DGGS\\data\\processed\\landsat_salinity_2022_full_year\n",
      "‚úÖ Boundary: d:\\Mekong_DGGS\\data\\raw\\DBSCL_Boundary_Clean.shp\n",
      "‚úÖ Output: d:\\Mekong_DGGS\\data\\processed\\landsat_salinity_2022_full_year\\salinity_h3_features_2022.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask as rasterio_mask\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "import h3\n",
    "\n",
    "# X√°c ƒë·ªãnh project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / 'data').exists() and (PROJECT_ROOT.parent / 'data').exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "SALINITY_DIR = PROJECT_ROOT / 'data' / 'processed' / 'landsat_salinity_2022_full_year'\n",
    "BOUNDARY_PATH = PROJECT_ROOT / 'data' / 'raw' / 'DBSCL_Boundary_Clean.shp'\n",
    "H3_GRID_OUT = SALINITY_DIR / 'h3_grid_dbscl_clean.geojson'\n",
    "OUTPUT_CSV = SALINITY_DIR / 'salinity_h3_features_2022.csv'\n",
    "\n",
    "SALINITY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'‚úÖ Project root: {PROJECT_ROOT}')\n",
    "print(f'‚úÖ Th∆∞ m·ª•c salinity: {SALINITY_DIR}')\n",
    "print(f'‚úÖ Boundary: {BOUNDARY_PATH}')\n",
    "print(f'‚úÖ Output: {OUTPUT_CSV}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf9e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ROI s·∫°ch: 1 feature\n",
      "‚úÖ H3 grid (res 7): 6959 √¥\n",
      "‚úÖ L∆∞u: d:\\Mekong_DGGS\\data\\processed\\landsat_salinity_2022_full_year\\h3_grid_dbscl_clean.geojson\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o H3 grid s·∫°ch (lo·∫°i ƒë·∫£o <600 km¬≤) v·ªõi chi·∫øn l∆∞·ª£c Buffer ‚Üí Polyfill ‚Üí Intersect\n",
    "RES = 7\n",
    "MIN_AREA_KM2 = 600\n",
    "H3_EDGE_LEN_METERS = {5: 8544.4, 6: 3229.6, 7: 1220.6, 8: 461.4, 9: 174.4, 10: 65.9}\n",
    "BUFFER_DIST = H3_EDGE_LEN_METERS.get(RES, 2000) * 2.0\n",
    "\n",
    "def clean_roi(roi_gdf, min_area_km2=600):\n",
    "    \"\"\"Lo·∫°i b·ªè ƒë·∫£o nh·ªè\"\"\"\n",
    "    gdf_metric = roi_gdf.to_crs(\"EPSG:32648\")\n",
    "    gdf_exploded = gdf_metric.explode(index_parts=True).reset_index(drop=True)\n",
    "    gdf_exploded[\"area_km2\"] = gdf_exploded.geometry.area / 1e6\n",
    "    gdf_clean = gdf_exploded[gdf_exploded[\"area_km2\"] > min_area_km2].copy()\n",
    "    gdf_dissolved = gdf_clean.dissolve()\n",
    "    return gdf_dissolved.to_crs(\"EPSG:4326\")\n",
    "\n",
    "def get_h3_funcs():\n",
    "    v = int(h3.__version__.split('.')[0])\n",
    "    return (h3.latlng_to_cell, h3.cell_to_boundary) if v >= 4 else (h3.geo_to_h3, h3.h3_to_geo_boundary)\n",
    "\n",
    "to_cell_func, to_boundary_func = get_h3_funcs()\n",
    "\n",
    "def generate_h3_grid(gdf, resolution):\n",
    "    \"\"\"Buffer ‚Üí Polyfill ‚Üí Intersect\"\"\"\n",
    "    gdf_metric = gdf.to_crs(\"EPSG:32648\")\n",
    "    gdf_buffered = gdf_metric.copy()\n",
    "    gdf_buffered[\"geometry\"] = gdf_metric.buffer(BUFFER_DIST)\n",
    "    gdf_buffered_ll = gdf_buffered.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    hex_set = set()\n",
    "    for _, row in gdf_buffered_ll.iterrows():\n",
    "        geom = row.geometry\n",
    "        geoms_list = geom.geoms if hasattr(geom, \"geoms\") else [geom]\n",
    "        for single_geom in geoms_list:\n",
    "            try:\n",
    "                if hasattr(h3, \"polygon_to_cells\"):\n",
    "                    from h3 import LatLngPoly\n",
    "                    outer = [(lat, lon) for lon, lat in single_geom.exterior.coords]\n",
    "                    holes = [[(lat, lon) for lon, lat in interior.coords] for interior in single_geom.interiors]\n",
    "                    hexes = h3.polygon_to_cells(LatLngPoly(outer, holes), resolution)\n",
    "                else:\n",
    "                    from shapely.geometry import mapping\n",
    "                    hexes = h3.polyfill(mapping(single_geom), resolution, geo_json_conformant=True)\n",
    "                hex_set.update(hexes)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Polyfill: {e}\")\n",
    "    \n",
    "    union_poly = unary_union(gdf.geometry.values)\n",
    "    valid_hexes, hex_polys = [], []\n",
    "    for h_idx in hex_set:\n",
    "        try:\n",
    "            boundary = to_boundary_func(h_idx)\n",
    "            poly_coords = [(lon, lat) for lat, lon in boundary] if hasattr(h3, \"cell_to_boundary\") else boundary\n",
    "            poly_geom = Polygon(poly_coords)\n",
    "            if poly_geom.intersects(union_poly):\n",
    "                valid_hexes.append(h_idx)\n",
    "                hex_polys.append(poly_geom)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return gpd.GeoDataFrame({\"h3_index\": valid_hexes}, geometry=hex_polys, crs=\"EPSG:4326\")\n",
    "\n",
    "# Load boundary v√† t·∫°o H3\n",
    "roi_raw = gpd.read_file(BOUNDARY_PATH)\n",
    "roi_clean = clean_roi(roi_raw, MIN_AREA_KM2)\n",
    "print(f\"‚úÖ ROI s·∫°ch: {len(roi_clean)} feature\")\n",
    "\n",
    "h3_grid = generate_h3_grid(roi_clean, RES)\n",
    "print(f\"‚úÖ H3 grid (res {RES}): {len(h3_grid)} √¥\")\n",
    "\n",
    "h3_grid.to_file(H3_GRID_OUT, driver=\"GeoJSON\")\n",
    "print(f\"‚úÖ L∆∞u: {H3_GRID_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56591be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H√†m x·ª≠ l√Ω salinity ƒë√£ s·∫µn s√†ng\n"
     ]
    }
   ],
   "source": [
    "def process_salinity(gdf, tiff_path):\n",
    "    \"\"\"T√≠nh salinity features cho t·ª´ng √¥ H3\"\"\"\n",
    "    results = []\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        gdf_crs = gdf.to_crs(src.crs)\n",
    "        for geom, h3_idx in zip(gdf_crs.geometry, gdf['h3_index']):\n",
    "            row = {\n",
    "                'h3_index': h3_idx,\n",
    "                'salinity_min': np.nan,\n",
    "                'salinity_max': np.nan,\n",
    "                'salinity_mean': np.nan,\n",
    "                'salinity_std': np.nan,\n",
    "                'pct_salinity_pixels': 0.0,\n",
    "                'pct_salinity_gte_0_2': 0.0,\n",
    "                'pct_salinity_gte_0_5': 0.0,\n",
    "                'pct_salinity_gte_1_0': 0.0\n",
    "            }\n",
    "            try:\n",
    "                masked, _ = rasterio_mask(src, [geom], crop=True)\n",
    "                data = masked[0].astype(float).flatten()\n",
    "                data_valid = data[(data >= -100) & (data <= 100) & ~np.isnan(data)]\n",
    "                total_pixels = data.size\n",
    "                \n",
    "                if data_valid.size > 0:\n",
    "                    row['salinity_min'] = float(np.nanmin(data_valid))\n",
    "                    row['salinity_max'] = float(np.nanmax(data_valid))\n",
    "                    row['salinity_mean'] = float(np.nanmean(data_valid))\n",
    "                    row['salinity_std'] = float(np.nanstd(data_valid))\n",
    "                    row['pct_salinity_pixels'] = (data_valid.size / total_pixels * 100) if total_pixels > 0 else 0.0\n",
    "                    row['pct_salinity_gte_0_2'] = (np.sum(data_valid >= 0.2) / data_valid.size * 100)\n",
    "                    row['pct_salinity_gte_0_5'] = (np.sum(data_valid >= 0.5) / data_valid.size * 100)\n",
    "                    row['pct_salinity_gte_1_0'] = (np.sum(data_valid >= 1.0) / data_valid.size * 100)\n",
    "            except:\n",
    "                pass\n",
    "            results.append(row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"‚úÖ H√†m x·ª≠ l√Ω salinity ƒë√£ s·∫µn s√†ng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef13330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ T√¨m th·∫•y 12 file raster\n",
      "  - 2022_M01_L89_Salinity.tif\n",
      "  - 2022_M02_L89_Salinity.tif\n",
      "  - 2022_M03_L89_Salinity.tif\n",
      "  - 2022_M04_L89_Salinity.tif\n",
      "  - 2022_M05_L89_Salinity.tif\n",
      "  ... v√† 7 file kh√°c\n"
     ]
    }
   ],
   "source": [
    "# Li·ªát k√™ t·∫•t c·∫£ file salinity .tif\n",
    "tif_files = sorted(glob.glob(str(SALINITY_DIR / '*.tif')))\n",
    "print(f'üìÅ T√¨m th·∫•y {len(tif_files)} file raster')\n",
    "for f in tif_files[:5]:\n",
    "    print(f'  - {os.path.basename(f)}')\n",
    "if len(tif_files) > 5:\n",
    "    print(f'  ... v√† {len(tif_files) - 5} file kh√°c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a06a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ki·ªÉm tra t·∫•t c·∫£ file .tif...\n",
      "\n",
      "‚úÖ File h·ª£p l·ªá: 12/12\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra v√† s·ª≠a file corrupt (n·∫øu c√≥)\n",
    "def validate_and_fix_tif(tif_path):\n",
    "    \"\"\"Ki·ªÉm tra file .tif, th·ª≠ fix n·∫øu corrupt\"\"\"\n",
    "    filename = os.path.basename(tif_path)\n",
    "    \n",
    "    # Test 1: Th·ª≠ m·ªü file basic\n",
    "    try:\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            _ = src.crs\n",
    "            _ = src.bounds\n",
    "        return True, \"OK\"\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Test 2: Th·ª≠ v·ªõi GDAL options kh√°c\n",
    "        try:\n",
    "            with rasterio.Env(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR'):\n",
    "                with rasterio.open(tif_path, OVERVIEW_LEVEL=0) as src:\n",
    "                    _ = src.crs\n",
    "            return True, \"Fixed with GDAL options\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return False, error_msg[:100]\n",
    "\n",
    "print(\"üîç Ki·ªÉm tra t·∫•t c·∫£ file .tif...\")\n",
    "valid_files = []\n",
    "corrupt_files = []\n",
    "\n",
    "for tif_path in tif_files:\n",
    "    filename = os.path.basename(tif_path)\n",
    "    is_valid, msg = validate_and_fix_tif(tif_path)\n",
    "    if is_valid:\n",
    "        valid_files.append(tif_path)\n",
    "        if msg != \"OK\":\n",
    "            print(f\"  ‚ö†Ô∏è  {filename}: {msg}\")\n",
    "    else:\n",
    "        corrupt_files.append((filename, msg))\n",
    "        print(f\"  ‚ùå {filename}: CORRUPT - {msg[:60]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ File h·ª£p l·ªá: {len(valid_files)}/{len(tif_files)}\")\n",
    "if corrupt_files:\n",
    "    print(f\"‚ùå File corrupt: {len(corrupt_files)}\")\n",
    "    print(\"\\nüí° Khuy·∫øn ngh·ªã: T·∫£i l·∫°i c√°c file corrupt t·ª´ ngu·ªìn ho·∫∑c x√≥a kh·ªèi th∆∞ m·ª•c\")\n",
    "\n",
    "# C·∫≠p nh·∫≠t danh s√°ch file ƒë·ªÉ x·ª≠ l√Ω\n",
    "tif_files = valid_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98f54558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ T·∫•t c·∫£ file ƒë·ªÅu h·ª£p l·ªá!\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch chi ti·∫øt file corrupt\n",
    "if corrupt_files:\n",
    "    print(\"üìä PH√ÇN T√çCH FILE CORRUPT:\\n\")\n",
    "    \n",
    "    # L·∫•y k√≠ch th∆∞·ªõc file h·ª£p l·ªá ƒë·ªÉ so s√°nh\n",
    "    valid_sizes = [os.path.getsize(f) for f in valid_files]\n",
    "    avg_size = np.mean(valid_sizes) if valid_sizes else 0\n",
    "    min_size = np.min(valid_sizes) if valid_sizes else 0\n",
    "    max_size = np.max(valid_sizes) if valid_sizes else 0\n",
    "    \n",
    "    print(f\"K√≠ch th∆∞·ªõc file h·ª£p l·ªá:\")\n",
    "    print(f\"  Trung b√¨nh: {avg_size/1024/1024:.2f} MB\")\n",
    "    print(f\"  Min-Max: {min_size/1024/1024:.2f} - {max_size/1024/1024:.2f} MB\\n\")\n",
    "    \n",
    "    for corrupt_file, error in corrupt_files:\n",
    "        corrupt_path = SALINITY_DIR / corrupt_file\n",
    "        if corrupt_path.exists():\n",
    "            corrupt_size = os.path.getsize(corrupt_path)\n",
    "            size_ratio = (corrupt_size / avg_size * 100) if avg_size > 0 else 0\n",
    "            \n",
    "            print(f\"‚ùå {corrupt_file}:\")\n",
    "            print(f\"   K√≠ch th∆∞·ªõc: {corrupt_size/1024/1024:.2f} MB ({size_ratio:.1f}% so v·ªõi TB)\")\n",
    "            print(f\"   L·ªói: {error[:100]}\")\n",
    "            \n",
    "            if size_ratio < 50:\n",
    "                print(f\"   ‚ö†Ô∏è  FILE CH∆ØA T·∫¢I XONG (ch·ªâ {size_ratio:.1f}%) - T·∫£i l·∫°i t·ª´ ngu·ªìn\")\n",
    "            elif size_ratio < 90:\n",
    "                print(f\"   ‚ö†Ô∏è  FILE B·ªä THI·∫æU D·ªÆ LI·ªÜU ({size_ratio:.1f}%) - T·∫£i l·∫°i ho·∫∑c export l·∫°i\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  FILE G·∫¶N ƒê·ª¶ ({size_ratio:.1f}%) nh∆∞ng header/metadata b·ªã l·ªói\")\n",
    "            \n",
    "            print(f\"   üí° Gi·∫£i ph√°p:\")\n",
    "            print(f\"      1. X√≥a: del {corrupt_path}\")\n",
    "            print(f\"      2. T·∫£i l·∫°i t·ª´ Google Drive/GEE\")\n",
    "            print(f\"      3. Ho·∫∑c b·ªè qua (ƒë√£ skip t·ª± ƒë·ªông)\\n\")\n",
    "else:\n",
    "    print(\"‚úÖ T·∫•t c·∫£ file ƒë·ªÅu h·ª£p l·ªá!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e72da657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ [1/12] 2022_M01_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [2/12] 2022_M02_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [3/12] 2022_M03_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [4/12] 2022_M04_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [5/12] 2022_M05_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [6/12] 2022_M06_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [7/12] 2022_M07_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [8/12] 2022_M08_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [9/12] 2022_M09_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [10/12] 2022_M10_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [11/12] 2022_M11_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "üîÑ [12/12] 2022_M12_L89_Salinity.tif\n",
      "   ‚úÖ 6959 √¥ x·ª≠ l√Ω xong\n",
      "\n",
      "‚úÖ L∆∞u: d:\\Mekong_DGGS\\data\\processed\\landsat_salinity_2022_full_year\\salinity_h3_features_2022.csv\n",
      "üìä T·ªïng: 83508 d√≤ng t·ª´ 12/12 file\n",
      "\n",
      "üìä Ph√¢n b·ªë theo th√°ng:\n",
      "   Th√°ng 01: 6959 d√≤ng\n",
      "   Th√°ng 02: 6959 d√≤ng\n",
      "   Th√°ng 03: 6959 d√≤ng\n",
      "   Th√°ng 04: 6959 d√≤ng\n",
      "   Th√°ng 05: 6959 d√≤ng\n",
      "   Th√°ng 06: 6959 d√≤ng\n",
      "   Th√°ng 07: 6959 d√≤ng\n",
      "   Th√°ng 08: 6959 d√≤ng\n",
      "   Th√°ng 09: 6959 d√≤ng\n",
      "   Th√°ng 10: 6959 d√≤ng\n",
      "   Th√°ng 11: 6959 d√≤ng\n",
      "   Th√°ng 12: 6959 d√≤ng\n",
      "\n",
      "üìä Preview 5 d√≤ng:\n",
      "          h3_index time  salinity_mean  salinity_std  pct_salinity_gte_0_2  pct_salinity_gte_1_0\n",
      "0  8765b5371ffffff   01       0.597343      0.104642             99.872449                   0.0\n",
      "1  8765b5262ffffff   01       0.394314      0.098543             97.656716                   0.0\n",
      "2  8765a2374ffffff   01       0.681155      0.087410             99.955720                   0.0\n",
      "3  8765a35b4ffffff   01       0.616323      0.049406             99.970571                   0.0\n",
      "4  8765a628effffff   01       0.527763      0.149161             97.841298                   0.0\n"
     ]
    }
   ],
   "source": [
    "# X·ª≠ l√Ω batch t·∫•t c·∫£ file v√† g·ªôp k·∫øt qu·∫£\n",
    "all_results = []\n",
    "failed_files = []\n",
    "\n",
    "for i, tif_path in enumerate(tif_files, 1):\n",
    "    filename = os.path.basename(tif_path)\n",
    "    print(f'üîÑ [{i}/{len(tif_files)}] {filename}')\n",
    "    try:\n",
    "        df = process_salinity(h3_grid, tif_path)\n",
    "        df['source_file'] = filename\n",
    "        all_results.append(df)\n",
    "        print(f'   ‚úÖ {len(df)} √¥ x·ª≠ l√Ω xong')\n",
    "    except Exception as e:\n",
    "        print(f'   ‚ùå L·ªói: {str(e)[:80]}...')\n",
    "        failed_files.append((filename, str(e)))\n",
    "\n",
    "if all_results:\n",
    "    merged = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # T·∫°o c·ªôt time t·ª´ source_file (VD: 2022_M01_L89_Salinity.tif -> 01)\n",
    "    import re\n",
    "    merged['time'] = merged['source_file'].str.extract(r'_M(\\d{2})_', expand=False)\n",
    "    \n",
    "    # B·ªè c·ªôt source_file\n",
    "    merged = merged.drop(columns=['source_file'])\n",
    "    \n",
    "    # S·∫Øp x·∫øp c·ªôt: h3_index, time, c√°c c·ªôt salinity\n",
    "    cols = ['h3_index', 'time'] + [c for c in merged.columns if c not in ['h3_index', 'time']]\n",
    "    merged = merged[cols]\n",
    "    \n",
    "    # L∆∞u CSV\n",
    "    merged.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f'\\n‚úÖ L∆∞u: {OUTPUT_CSV}')\n",
    "    print(f'üìä T·ªïng: {len(merged)} d√≤ng t·ª´ {len(all_results)}/{len(tif_files)} file')\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f'\\n‚ö†Ô∏è  {len(failed_files)} file b·ªã l·ªói:')\n",
    "        for fname, err in failed_files:\n",
    "            print(f'   - {fname}')\n",
    "    \n",
    "    # Th·ªëng k√™ nhanh\n",
    "    print('\\nüìä Ph√¢n b·ªë theo th√°ng:')\n",
    "    for month, count in merged['time'].value_counts().sort_index().items():\n",
    "        print(f'   Th√°ng {month}: {count} d√≤ng')\n",
    "    \n",
    "    print('\\nüìä Preview 5 d√≤ng:')\n",
    "    preview = merged[['h3_index', 'time', 'salinity_mean', 'salinity_std', \n",
    "                      'pct_salinity_gte_0_2', 'pct_salinity_gte_1_0']].head()\n",
    "    print(preview.to_string())\n",
    "else:\n",
    "    print('‚ùå Kh√¥ng c√≥ file .tif n√†o x·ª≠ l√Ω th√†nh c√¥ng')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
